import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from io import StringIO

# ==============================
# Configuraci√≥n de fechas
# ==============================
start_date = datetime(2025, 10, 7)
end_date = datetime.today()

# ==============================
# Scraping con requests
# ==============================
all_data = []
current_date = start_date

while current_date <= end_date:
    date_str = current_date.strftime("%Y-%m-%d")
    url = f"http://rmcab.ambientebogota.gov.co/Report/HourlyReports?id=1&UserDateString={date_str}"
    
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        
        soup = BeautifulSoup(r.text, "lxml")
        
        # Extraer t√≠tulos (estaciones) y tablas
        titles = [h2.get_text(strip=True) for h2 in soup.find_all("h2")]
        tables = soup.find_all("table")
        
        # Emparejar t√≠tulo con tabla
        for title, table in zip(titles, tables):
            df = pd.read_html(StringIO(str(table)))[0]
            df["Fecha"] = date_str
            df["Estacion"] = title  # ‚úÖ asociar t√≠tulo
            all_data.append(df)
        
        print(f"‚úÖ {date_str} - {len(tables)} tablas descargadas ({len(titles)} t√≠tulos)")
    
    except Exception as e:
        print(f"‚ö†Ô∏è Error en {date_str}: {e}")
    
    current_date += timedelta(days=1)

# ==============================
# CONSOLIDAR Y GUARDAR RESULTADOS
# ==============================
if all_data:
    final_df = pd.concat(all_data, ignore_index=True)

    # Limpieza adicional por si hay valores residuales con coma
    for col in final_df.columns:
        if final_df[col].astype(str).str.contains(r'\d', regex=True).any():
            final_df[col] = (
                final_df[col]
                .astype(str)
                .str.replace(".", "", regex=False)   # elimina separador de miles
                .str.replace(",", ".", regex=False)  # convierte coma en punto
            )
            final_df[col] = pd.to_numeric(final_df[col], errors="ignore")


# ==============================
# Guardar resultados
# ==============================
if all_data:
    final_df = pd.concat(all_data, ignore_index=True)
    final_df.to_csv("rmcab_reportes.csv", index=False, encoding="utf-8-sig")
    print("üìÅ Archivo guardado como rmcab_reportes.csv")
else:
    print("‚ö†Ô∏è No se extrajo ninguna tabla")
